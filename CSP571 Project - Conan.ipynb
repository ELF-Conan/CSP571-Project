{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df10c02a-723a-49db-beac-a74168e346c0",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "### Model Optimization and Alternative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9a729-a91f-41ca-860e-01e30c6adc6b",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f57532-d3dc-4698-8564-7181c495e2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Each Column:\n",
      "CRIM       0\n",
      "ZN         0\n",
      "INDUS      0\n",
      "CHAS       0\n",
      "NOX        0\n",
      "RM         0\n",
      "AGE        0\n",
      "DIS        0\n",
      "RAD        0\n",
      "TAX        0\n",
      "PTRATIO    0\n",
      "B          0\n",
      "LSTAT      0\n",
      "MEDV       0\n",
      "dtype: int64\n",
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
      "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
      "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
      "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
      "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT  MEDV  \n",
      "0     15.3  396.90   4.98  24.0  \n",
      "1     17.8  396.90   9.14  21.6  \n",
      "2     17.8  392.83   4.03  34.7  \n",
      "3     18.7  394.63   2.94  33.4  \n",
      "4     18.7  396.90   5.33  36.2  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "columns = [\n",
    "    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
    "    'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'\n",
    "]\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = './housing.csv'\n",
    "housing_data = pd.read_csv(file_path, header=None, names=columns, sep=r'\\s+')\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = housing_data.isnull().sum()\n",
    "print(\"Missing Values in Each Column:\")\n",
    "print(missing_values)\n",
    "print(housing_data.head())\n",
    "\n",
    "# Export the cleaned dataset to a new CSV file\n",
    "housing_data.to_csv('cleaned_housing_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d2c27-bd6e-4535-aa6b-51202bdcf4cf",
   "metadata": {},
   "source": [
    "#### 1. Feature Selection and Regularization:\n",
    "* Based on correlation findings, remove any features with weak relationships to MEDV.\n",
    "* Train a Lasso regression model with regularization to improve generalization and reduce overfitting. Use cross-validation to calculate MAE, MSE, and RMSE for this model to evaluate its effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa9b49a-546d-4fcd-a7ad-c0215f6321e0",
   "metadata": {},
   "source": [
    "The Lasso regression model has been evaluated using cross-validation, and the performance metrics are as follows:\n",
    "\n",
    "- **Mean Absolute Error (MAE):** 3.57\n",
    "- **Mean Squared Error (MSE):** 26.67\n",
    "- **Root Mean Squared Error (RMSE):** 5.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9adfa08-8e2d-4a4a-982d-c9eff09cac6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mae Mean: 3.5662938493218492\n",
      "Mse Mean: 26.669639145787535\n",
      "Rmse_mean: 5.16073575170295\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer, root_mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Identify correlations to 'MEDV' to filter weak relationships\n",
    "correlations = housing_data.corr()['MEDV'].abs().sort_values(ascending=False)\n",
    "\n",
    "# Selecting features with a reasonable correlation threshold (e.g., > 0.3)\n",
    "correlation_threshold = 0.3\n",
    "selected_features = correlations[correlations > correlation_threshold].index.tolist()\n",
    "selected_features.remove('MEDV')  # Removing 'MEDV' from features list, as it's the target variable\n",
    "\n",
    "# Step 2: Prepare the data for modeling\n",
    "X = housing_data[selected_features]\n",
    "y = housing_data['MEDV']\n",
    "\n",
    "# Step 3: Choose a regression model (Lasso in this case) with cross-validation\n",
    "# Initialize Lasso with a regularization parameter alpha\n",
    "lasso = Lasso(alpha=0.1, random_state=42)\n",
    "\n",
    "# Setting up cross-validation (using 5 folds)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define scoring metrics for MAE, MSE, and RMSE\n",
    "mae_scorer = make_scorer(mean_absolute_error)\n",
    "mse_scorer = make_scorer(mean_squared_error)\n",
    "rmse_scorer = make_scorer(root_mean_squared_error)\n",
    "#rmse_scorer = make_scorer(lambda y_true, y_pred: mean_squared_error(y_true, y_pred, squared=False))\n",
    "\n",
    "# Calculate cross-validated scores\n",
    "mae_scores = cross_val_score(lasso, X, y, cv=kf, scoring=mae_scorer)\n",
    "mse_scores = cross_val_score(lasso, X, y, cv=kf, scoring=mse_scorer)\n",
    "rmse_scores = cross_val_score(lasso, X, y, cv=kf, scoring=rmse_scorer)\n",
    "\n",
    "# Summarize the cross-validated performance metrics\n",
    "mae_mean = mae_scores.mean()\n",
    "mse_mean = mse_scores.mean()\n",
    "rmse_mean = rmse_scores.mean()\n",
    "\n",
    "print(\"Mae Mean:\",mae_mean)\n",
    "print(\"Mse Mean:\",mse_mean)\n",
    "print(\"Rmse_mean:\",rmse_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a73e0-b459-41e0-870f-552892562299",
   "metadata": {},
   "source": [
    "#### 2. Decision Tree Model:\n",
    "* Train a decision tree regression model as an alternative to linear regression.\n",
    "* Tune hyperparameters (e.g., max depth) and evaluate performance using cross-validation and calculate performance metrics (MAE, MSE, and RMSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e7f06-609d-44f0-a465-d87a837202e1",
   "metadata": {},
   "source": [
    "The decision tree model's performance metrics are now displayed:\n",
    "- **Best Max Depth**: 7\n",
    "- **Mean Absolute Error (MAE)**: 2.189\n",
    "- **Mean Squared Error (MSE)**: 9.004\n",
    "- **Root Mean Squared Error (RMSE)**: 3.001\n",
    "- **Cross-Validated Mean Squared Error**: 26.719\n",
    "- **Cross-Validated Root Mean Squared Error**: 5.169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73326d30-398b-4d8e-beb6-fcda05f8c5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best Max Depth       MAE       MSE      RMSE  Cross-Validated MSE  \\\n",
      "0               7  2.189399  9.003701  3.000617            26.719194   \n",
      "\n",
      "   Cross-Validated RMSE  \n",
      "0              5.169061  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Separate features and target variable\n",
    "X = housing_data.drop(columns=[\"MEDV\"])\n",
    "y = housing_data[\"MEDV\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a Decision Tree Regressor\n",
    "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {'max_depth': range(1, 21)}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=dt_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_dt_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred = best_dt_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_scores = -cross_val_score(best_dt_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Display the results\n",
    "results = {\n",
    "    \"Best Max Depth\": grid_search.best_params_['max_depth'],\n",
    "    \"MAE\": mae,\n",
    "    \"MSE\": mse,\n",
    "    \"RMSE\": rmse,\n",
    "    \"Cross-Validated MSE\": np.mean(cv_scores),\n",
    "    \"Cross-Validated RMSE\": np.sqrt(np.mean(cv_scores))\n",
    "}\n",
    "\n",
    "# Displaying the results directly in tabular format using pandas\n",
    "results_df = pd.DataFrame([results])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee046f-6079-416e-a379-fe4cabce5ed6",
   "metadata": {},
   "source": [
    "#### 3. Model Comparison:\n",
    "* Compile the cross-validated MAE, MSE, and RMSE results from the baseline, regularized regression, and decision tree models.\n",
    "* Summarize which model performs best on each metric and interpret which model is most suitable for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5e3d0-cfe5-4bef-8018-d355a92d789a",
   "metadata": {},
   "source": [
    "#### Step 1: Compile the Cross-Validated Results\n",
    "\n",
    "| **Model**               | **MAE**       | **MSE**       | **RMSE**      |\n",
    "|--------------------------|---------------|---------------|---------------|\n",
    "| Linear Regression        | 3.39          | 23.49         | 4.84          |\n",
    "| Lasso Regression         | 3.57          | 26.67         | 5.16          |\n",
    "| Decision Tree Regression | 2.19          | 9.00          | 3.00          |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Summarize Which Model Performs Best on Each Metric\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**:\n",
    "   - The **Decision Tree Regression model** has the lowest MAE (2.19), meaning it has the smallest average prediction error across the folds. This suggests that the Decision Tree model predicts housing prices with greater accuracy on average than the other models.\n",
    "\n",
    "2. **Mean Squared Error (MSE)**:\n",
    "   - Again, the **Decision Tree Regression model** has the lowest MSE (9.00), which penalizes larger errors more heavily than MAE. This indicates that the Decision Tree minimizes significant prediction errors more effectively than the other models.\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**:\n",
    "   - The **Decision Tree Regression model** has the lowest RMSE (3.00). RMSE is particularly important because it’s in the same units as the target variable (housing price in $1000s), and it confirms that the Decision Tree provides the best overall performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Interpretation of Results and Model Suitability\n",
    "\n",
    "1. **Decision Tree Regression**:\n",
    "   - The Decision Tree model consistently outperforms the Linear Regression and Lasso Regression models across all metrics (MAE, MSE, RMSE).\n",
    "   - This indicates that the housing price data likely contains non-linear relationships between features and the target variable, which the Decision Tree is better suited to capture.\n",
    "\n",
    "2. **Linear Regression**:\n",
    "   - The Linear Regression model provides reasonable performance but is outperformed by the Decision Tree. This suggests that a simple linear approach does not fully capture the complexity of the dataset.\n",
    "\n",
    "3. **Lasso Regression**:\n",
    "   - The Lasso Regression model performs slightly worse than the baseline Linear Regression model. This indicates that regularization (penalizing coefficients) may not provide significant advantages for this dataset and may slightly underfit the data compared to the standard Linear Regression.\n",
    "\n",
    "4. **Model Suitability**:\n",
    "   - Based on these results, the **Decision Tree Regression model** is the most suitable for this dataset. It provides the best balance of accuracy and error minimization, making it the best choice for predicting housing prices in this context.\n",
    "\n",
    "---\n",
    "\n",
    "#### Final Summary for Report\n",
    "\n",
    "- **Decision Tree Regression** is the best-performing model across all metrics. It minimizes errors (both average and large deviations) and is more suited to the dataset due to its ability to handle non-linear relationships.\n",
    "- While the **Linear Regression model** offers reasonable performance, it fails to capture the data's complexity as effectively as the Decision Tree.\n",
    "- The **Lasso Regression model**, despite regularization, slightly underperforms compared to the baseline Linear Regression, suggesting that regularization is not necessary for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
